# Adam v/s Gradient Descent

Let us begin by understanding the principle behind Gradient Descent.

## Gradient Descent:
Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.
The aim of Gradient Descent is to alter the weights in such a way that we are able to reduce the cost. This allows the Neural Network to update its weights thereby allowing it to learn and understand the data it is given.

### Featrues of a Gradient Descent:

#### 1. Loss Section:
