# Adam v/s Gradient Descent

Let us begin by understanding the principle behind Gradient Descent.

## Gradient Descent:
Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. 